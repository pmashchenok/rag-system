# RAG-system
Проект представляет собой готовый прототип RAG-системы (backend + ML). Прототип состоит из двух микросервисов, взаимодействующих с векторной базой данных. Проект может быть запущен локально через взаимодействие с FastAPI, либо может быть собран образ и контейнер приложения через Docker. Сервис индексирования через метод POST принимает json файл с документами, очищает их, вычисляет эмбеддинги очищенных документов и записывает их в Chroma DB. Сервис запроса принимает от пользователя запрос, ищет в БД релевантные документы и на их основе формирует промпт для модели: запрос от пользователя + контекст. Для проверки работоспособности добавлены небольшие тестовые json файлы в директории data.

## Архитектурная схема
<img src="https://github.com/user-attachments/assets/8c609759-de24-4e10-ac75-8719f424fd3e" width="600" height="350"/>

## Клонирование репозитория
```bash
git clone https://github.com/pmashchenok/rag-system
```
## Локальный запуск приложения
### Сервис индексирования
Для запуска сервиса Индексирования в терминале пишем
```bash
uvicorn src.indexing_service.api.indexing_service:app
```
После этого мы можем отправлять сервису запросы. Например, пишем в cmd:
```bash
curl -X POST -H "Content-Type: application/json" --data-binary @data/test.json http://localhost:8001/indexing
```
Сервер отправляет пользователю сообщение о количестве проиндексировавнных документов.
### Сервис запросов
Для запуска сервиса Запросов в терминал пишем
```bash
uvicorn src.query_service.api.query_service:app
```
Теперь можно отправлять сервису запросы. Например, пишем в cmd:
```bash
curl --header "Content-Type: application/json" --request POST --data "{\"text\":\"Катастрофа самолета ТУ\"}" http://localhost:8000/query
```
По итогу, сервер отправляет пользователю ответ от LLM с учетом предоставленного контекста.

## Запуск через Docker
Для создания образа, контейнера и его запуска в cmd пишем
```bash
docker-compose up --build
```
Теперь можно отправлять сервисам запросы, аналогично, как это делалось локально



